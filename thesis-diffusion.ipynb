{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986c69bb",
   "metadata": {},
   "source": [
    "# Research notebook for backdoor attacks on audio generative diffusion models\n",
    "The diffusion model used in this notebook was based on a model from an assignment for week 11 of the 2023 Deep Learning course (NWI-IMC070) of the Radboud University.\n",
    "# Here is the initial code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f3076",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import sklearn.preprocessing\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059cbc38",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print(device)\n",
    "print(str(torchaudio.list_audio_backends()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# parameter setting from paper Denoising Diffusion Probabilistic Models\n",
    "diffusion_steps = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, diffusion_steps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "filename = \"thesis-diffusion-clean-model\"\n",
    "batch_size = 1\n",
    "samplerate = 16000\n",
    "new_samplerate = 3000\n",
    "n_fft=100 #400 was default\n",
    "win_length = n_fft #Default: n_fft\n",
    "hop_length = win_length // 2 #Default: win_length // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0801bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset (number of spoken commands)\n",
    "# num_classes = 35\n",
    "datalocation = \"/vol/csedu-nobackup/project/mnederlands/data\"\n",
    "modellocation = \"./saves\"\n",
    "os.makedirs(modellocation, exist_ok=True)\n",
    "os.makedirs(datalocation, exist_ok=True)\n",
    "# Load the data\n",
    "speech_commands_data = torchaudio.datasets.SPEECHCOMMANDS(root=datalocation, download=True)\n",
    "train_size = int(0.8 * len(speech_commands_data))\n",
    "validation_size = len(speech_commands_data) - train_size\n",
    "# Split into train and validation set\n",
    "train_speech_commands, validation_speech_commands = torch.utils.data.random_split(speech_commands_data, [train_size, validation_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fcd7ae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Function to pad waveforms to a specific length\n",
    "def pad_waveform(waveform, target_length):\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padded_waveform = F.pad(waveform, (0, target_length - current_length), mode='constant', value=0)\n",
    "        return padded_waveform\n",
    "    else:\n",
    "        return waveform\n",
    "# Define a transform to convert waveform to spectrogram\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.Resample(orig_freq=samplerate, new_freq=new_samplerate),\n",
    "    torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec92c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Initialization of label encoder\n",
    "le = sklearn.preprocessing.LabelEncoder() \n",
    "labels = np.ravel([row[2:3] for row in train_speech_commands])\n",
    "le.fit(labels)\n",
    "joblib.dump(le, modellocation + '/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731cd1a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Pad waveforms in train set and apply transform\n",
    "train_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in train_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    train_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "# Pad waveforms in validation set and apply transform\n",
    "validation_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in validation_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    validation_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "resize_h, resize_w = spectrogram[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c21d921",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_speech_commands_padded, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_speech_commands_padded, batch_size=1000)\n",
    "#torch.save(train_loader, modellocation + '/train_loader.pth')\n",
    "#torch.save(validation_loader, modellocation + '/validation_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a20731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize spectrogram\n",
    "def show_spectrogram(spectrogram, title):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # Compute the magnitude or logarithm of the spectrogram\n",
    "    magnitude_spectrogram = torch.abs(spectrogram)\n",
    "    log_magnitude_spectrogram = torch.log2(magnitude_spectrogram + 1e-9)  # Add small epsilon to avoid log(0)\n",
    "    # Display the magnitude or logarithm of the spectrogram\n",
    "    plt.imshow(log_magnitude_spectrogram[0], aspect='auto', origin='lower')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def generate_noisy_samples(x_0, beta):\n",
    "    '''\n",
    "    Create noisy samples for the minibatch x_0.\n",
    "    Return the noisy image, the noise, and the time for each sample.\n",
    "    '''\n",
    "    x_0 = x_0.to(device)  # Ensure the input tensor is on GPU\n",
    "    beta = beta.to(device)  # Ensure beta is on GPU\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0).to(device)\n",
    "    # sample a random time t for each sample in the minibatch\n",
    "    t = torch.randint(beta.shape[0], size=(x_0.shape[0],), device=x_0.device)\n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    # Add the noise to each sample\n",
    "    x_t = torch.sqrt(alpha_bar[t, None, None, None]) * x_0 + \\\n",
    "          torch.sqrt(1 - alpha_bar[t, None, None, None]) * noise\n",
    "    return x_t, noise, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net code adapted from: https://github.com/milesial/Pytorch-UNet\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value\n",
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s[0] * self.num_s[1]).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s[0], self.num_s[1])\n",
    "        return x\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class UNetConditional(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, n_classes=35, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa1 = SAWrapper(256, [int(resize_h/4), int(resize_w/4)])\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(256, 512 // factor)\n",
    "        self.sa2 = SAWrapper(256, [int(resize_h/8), int(resize_w/8)]) #\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.sa3 = SAWrapper(128, [int(resize_h/4), int(resize_w/4)])\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, c_out)\n",
    "        self.label_embedding = nn.Embedding(n_classes, 256)\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def label_encoding(self, label, channels, embed_size):\n",
    "        return self.label_embedding(label)[:, :channels, None, None].repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def forward(self, x, t, label):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, 128, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 128, (int(resize_h/2), int(resize_w/2)))\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, 256, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 256, (int(resize_h/4), int(resize_w/4)))\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, 256, (resize_h/8, int(resize_w/8))) + self.label_encoding(label, 256, (resize_h/8, int(resize_w/8)))\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, 128, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 128, (int(resize_h/4), int(resize_w/4)))\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, 64, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 64, (int(resize_h/2), int(resize_w/2)))\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, 64, (int(resize_h), int(resize_w))) + self.label_encoding(label, 64, (int(resize_h), int(resize_w)))\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conditional(model, beta, num_epochs=10, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # generate a noisy minibatch\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            # use the model to estimate the noise\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            # compute the difference between the noise and the estimated noise\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Track our progress\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0])\n",
    "        train_loss = metric[0] / metric[1]\n",
    "        # Compute test loss\n",
    "        validation_loss = test_conditional(model, validation_loader, beta)\n",
    "        # Plot\n",
    "        #animator.add(epoch + 1, (train_loss, validation_loss))\n",
    "        print(\"epoch:\" + str(epoch) + \"train_loss:\" + str(train_loss) +  \"validation_loss:\" + str(validation_loss))\n",
    "    print(f'training loss {train_loss:.3g}, validation loss {validation_loss:.3g}')\n",
    "    torch.save(model.state_dict(), modellocation + \"/\" + filename + \".pth\")\n",
    "def test_conditional(model, validation_loader, beta):\n",
    "    metric = d2l.Accumulator(2)\n",
    "    model.eval()\n",
    "    for x, y in validation_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0])\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"begin training, batchsize:\" + str(batch_size))\n",
    "model_conditional = UNetConditional().to(device)\n",
    "train_conditional(model_conditional, beta, num_epochs=10, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model_conditional(x, model, beta, label):\n",
    "    # keep track of x at different time steps\n",
    "    x_hist = []\n",
    "    with torch.no_grad():\n",
    "        c = (torch.ones(x.shape[0]) * label).long().to(device)\n",
    "        # loop over all time steps in reverse order\n",
    "        for i in reversed(range(0, beta.shape[0])):\n",
    "            # copy the time step for each sample in the minibatch\n",
    "            t = (torch.ones(x.shape[0]) * i).long().to(device)\n",
    "            # generate random noise for early time steps\n",
    "            z = torch.randn_like(x) if i > 0 else torch.zeros_like(x)\n",
    "            # define sigma as suggested in the paper\n",
    "            sigma = torch.sqrt(beta[i])\n",
    "            # compute the next x\n",
    "            x = (1 / torch.sqrt(alpha[i])) * \\\n",
    "                (x - ((1 - alpha[i]) / torch.sqrt(1 - alpha_bar[i])) * model(x, t, c)) + \\\n",
    "                sigma * z\n",
    "            if i % 100 == 0:\n",
    "                x_hist.append(x.detach().cpu().numpy())\n",
    "    return x, x_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eedebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotfix\n",
    "x_0, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_hist(x_hist):\n",
    "    # plot the generated images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(x_hist)):\n",
    "        for j in range(10):\n",
    "            plt.subplot(10, len(x_hist), j * len(x_hist) + i + 1)\n",
    "            spectrogram = x_hist[i][j]\n",
    "            plt.imshow(x_hist[i][j])\n",
    "            np.log2((spectrogram + 1e-10), aspect='auto', origin='lower')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Spectrogram {i * 10 + j}')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize spectrogram\n",
    "def show_spectrogram(spectrogram, title):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(spectrogram.log2()[0], aspect='auto', origin='lower')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222586f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le.classes_)\n",
    "print(len(le.classes_))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
