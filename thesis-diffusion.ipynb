{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106a6d5f",
   "metadata": {},
   "source": [
    "# Research notebook for backdoor attacks on audio generative diffusion models\n",
    "The diffusion model used in this notebook was based on a model from an assignment for week 11 of the 2023 Deep Learning course (NWI-IMC070) of the Radboud University.\n",
    "# Here is the initial code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c70ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import sklearn.preprocessing\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81eda538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "['soundfile']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print(device)\n",
    "print(str(torchaudio.list_audio_backends()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ff3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "diffusion_steps = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, diffusion_steps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "filename = \"thesis-diffusion-clean-model\"\n",
    "batch_size = 1\n",
    "samplerate = 16000\n",
    "new_samplerate = 8000\n",
    "n_fft=200 #400 was default\n",
    "win_length = n_fft #Default: n_fft\n",
    "hop_length = win_length // 2 #Default: win_length // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e5efe63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize_h = n_fft // 2 + 1\n",
    "#resize_w = math.ceil((new_samplerate - n_fft) / hop_length) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b34f7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datalocation = \"../Data\"\n",
    "datalocation = \"/vol/csedu-nobackup/project/mnederlands/data\"\n",
    "modellocation = \"./saves\"\n",
    "os.makedirs(modellocation, exist_ok=True)\n",
    "os.makedirs(datalocation, exist_ok=True)\n",
    "# Number of classes in the dataset (number of spoken commands)\n",
    "num_classes = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b583435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# ### Audio data\n",
    "# Load the data\n",
    "speech_commands_data = torchaudio.datasets.SPEECHCOMMANDS(root=datalocation, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33dbc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(speech_commands_data))\n",
    "validation_size = len(speech_commands_data) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90a1b2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Split into train and validation set\n",
    "train_speech_commands, validation_speech_commands = torch.utils.data.random_split(speech_commands_data, [train_size, validation_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01eb709b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to pad waveforms to a specific length\n",
    "def pad_waveform(waveform, target_length):\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padded_waveform = F.pad(waveform, (0, target_length - current_length), mode='constant', value=0)\n",
    "        return padded_waveform\n",
    "    else:\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39690c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert waveform to spectrogram\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.Resample(orig_freq=samplerate, new_freq=new_samplerate),\n",
    "    torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523bd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialization of label encoder\n",
    "le = sklearn.preprocessing.LabelEncoder() \n",
    "labels = np.ravel([row[2:3] for row in train_speech_commands])\n",
    "le.fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e33a409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101, 81])\n"
     ]
    }
   ],
   "source": [
    "# Pad waveforms in train set and apply transform\n",
    "train_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in train_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    train_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "# Pad waveforms in validation set and apply transform\n",
    "validation_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in validation_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    validation_speech_commands_padded.append([spectrogram, le.transform([label])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "31638c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101, 81])\n",
      "101\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "print(spectrogram.shape)\n",
    "resize_h, resize_w = spectrogram[0].shape\n",
    "print(resize_h)\n",
    "print(resize_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "21c129b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_speech_commands_padded, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_speech_commands_padded, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5fb6c453",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saves/label_encoder.pkl']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.save(train_loader, modellocation + '/train_loader.pth')\n",
    "#torch.save(validation_loader, modellocation + '/validation_loader.pth')\n",
    "joblib.dump(le, modellocation + '/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ee6c9a03",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# parameter setting from paper Denoising Diffusion Probabilistic Models\n",
    "# Function to visualize spectrogram\n",
    "def show_spectrogram(spectrogram, title):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # Compute the magnitude or logarithm of the spectrogram\n",
    "    magnitude_spectrogram = torch.abs(spectrogram)\n",
    "    log_magnitude_spectrogram = torch.log2(magnitude_spectrogram + 1e-9)  # Add small epsilon to avoid log(0)\n",
    "    # Display the magnitude or logarithm of the spectrogram\n",
    "    plt.imshow(log_magnitude_spectrogram[0], aspect='auto', origin='lower')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "56fafa78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_noisy_samples(x_0, beta):\n",
    "    '''\n",
    "    Create noisy samples for the minibatch x_0.\n",
    "    Return the noisy image, the noise, and the time for each sample.\n",
    "    '''\n",
    "    x_0 = x_0.to(device)  # Ensure the input tensor is on GPU\n",
    "    beta = beta.to(device)  # Ensure beta is on GPU\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0).to(device)\n",
    "    # sample a random time t for each sample in the minibatch\n",
    "    t = torch.randint(beta.shape[0], size=(x_0.shape[0],), device=x_0.device)\n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    # Add the noise to each sample\n",
    "    x_t = torch.sqrt(alpha_bar[t, None, None, None]) * x_0 + \\\n",
    "          torch.sqrt(1 - alpha_bar[t, None, None, None]) * noise\n",
    "    return x_t, noise, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9507df51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# U-Net code adapted from: https://github.com/milesial/Pytorch-UNet\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c4a1fce1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s[0] * self.num_s[1]).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s[0], self.num_s[1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "18b532b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "37e1989f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13f95b3a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "35986b58",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "99e2460f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class UNetConditional(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, n_classes=35, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa1 = SAWrapper(256, [round(resize_h/4), round(resize_w/4)])\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(256, 512 // factor)\n",
    "        self.sa2 = SAWrapper(256, [int(resize_h/8), int(resize_w/8)]) #\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.sa3 = SAWrapper(128, [round(resize_h/4), round(resize_w/4)])\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, c_out)\n",
    "        self.label_embedding = nn.Embedding(n_classes, 256)\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def label_encoding(self, label, channels, embed_size):\n",
    "        return self.label_embedding(label)[:, :channels, None, None].repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def forward(self, x, t, label):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, 128, (round(resize_h/2), round(resize_w/2))) + self.label_encoding(label, 128, (round(resize_h/2), round(resize_w/2)))\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, 256, (round(resize_h/4), round(resize_w/4))) + self.label_encoding(label, 256, (round(resize_h/4), round(resize_w/4)))\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, 256, (resize_h/8, round(resize_w/8))) + self.label_encoding(label, 256, (resize_h/8, round(resize_w/8)))\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, 128, (round(resize_h/4), round(resize_w/4))) + self.label_encoding(label, 128, (round(resize_h/4), round(resize_w/4)))\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, 64, (round(resize_h/2), round(resize_w/2))) + self.label_encoding(label, 64, (round(resize_h/2), round(resize_w/2)))\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, 64, (round(resize_h), round(resize_w))) + self.label_encoding(label, 64, (round(resize_h), round(resize_w)))\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ccf406e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_conditional(model, beta, num_epochs=10, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # generate a noisy minibatch\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            # use the model to estimate the noise\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            # compute the difference between the noise and the estimated noise\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Track our progress\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0])\n",
    "        train_loss = metric[0] / metric[1]\n",
    "        # Compute test loss\n",
    "        validation_loss = test_conditional(model, validation_loader, beta)\n",
    "        # Plot\n",
    "        #animator.add(epoch + 1, (train_loss, validation_loss))\n",
    "        print(\"epoch:\" + str(epoch) + \"train_loss:\" + str(train_loss) +  \"validation_loss:\" + str(validation_loss))\n",
    "    print(f'training loss {train_loss:.3g}, validation loss {validation_loss:.3g}')\n",
    "    torch.save(model.state_dict(), modellocation + \"/\" + filename + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6b2b2bc6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_conditional(model, validation_loader, beta):\n",
    "    metric = d2l.Accumulator(2)\n",
    "    model.eval()\n",
    "    for x, y in validation_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0])\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dc7faf52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training, batchsize:1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin training, batchsize:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_size))\n\u001b[0;32m      2\u001b[0m model_conditional \u001b[38;5;241m=\u001b[39m UNetConditional()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_conditional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_conditional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[161], line 13\u001b[0m, in \u001b[0;36mtrain_conditional\u001b[1;34m(model, beta, num_epochs, lr)\u001b[0m\n\u001b[0;32m     11\u001b[0m x_t, noise, sampled_t \u001b[38;5;241m=\u001b[39m generate_noisy_samples(x, beta\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# use the model to estimate the noise\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m estimated_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# compute the difference between the noise and the estimated noise\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(estimated_noise, noise)\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[160], line 38\u001b[0m, in \u001b[0;36mUNetConditional.forward\u001b[1;34m(self, x, t, label)\u001b[0m\n\u001b[0;32m     36\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x2) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(t, \u001b[38;5;241m256\u001b[39m, (\u001b[38;5;28mround\u001b[39m(resize_h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mround\u001b[39m(resize_w\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoding(label, \u001b[38;5;241m256\u001b[39m, (\u001b[38;5;28mround\u001b[39m(resize_h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mround\u001b[39m(resize_w\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)))\n\u001b[0;32m     37\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa1(x3)\n\u001b[1;32m---> 38\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(t, \u001b[38;5;241m256\u001b[39m, (resize_h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;28mround\u001b[39m(resize_w\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoding(label, \u001b[38;5;241m256\u001b[39m, (resize_h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;28mround\u001b[39m(resize_w\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m)))\n\u001b[0;32m     39\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa2(x4)\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(x4, x3) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(t, \u001b[38;5;241m128\u001b[39m, (\u001b[38;5;28mround\u001b[39m(resize_h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mround\u001b[39m(resize_w\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoding(label, \u001b[38;5;241m128\u001b[39m, (\u001b[38;5;28mround\u001b[39m(resize_h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mround\u001b[39m(resize_w\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[120], line 10\u001b[0m, in \u001b[0;36mDown.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"begin training, batchsize:\" + str(batch_size))\n",
    "model_conditional = UNetConditional().to(device)\n",
    "train_conditional(model_conditional, beta, num_epochs=10, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683bca1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sample_from_model_conditional(x, model, beta, label):\n",
    "    # keep track of x at different time steps\n",
    "    x_hist = []\n",
    "    with torch.no_grad():\n",
    "        c = (torch.ones(x.shape[0]) * label).long().to(device)\n",
    "        # loop over all time steps in reverse order\n",
    "        for i in reversed(range(0, beta.shape[0])):\n",
    "            # copy the time step for each sample in the minibatch\n",
    "            t = (torch.ones(x.shape[0]) * i).long().to(device)\n",
    "            # generate random noise for early time steps\n",
    "            z = torch.randn_like(x) if i > 0 else torch.zeros_like(x)\n",
    "            # define sigma as suggested in the paper\n",
    "            sigma = torch.sqrt(beta[i])\n",
    "            # compute the next x\n",
    "            x = (1 / torch.sqrt(alpha[i])) * \\\n",
    "                (x - ((1 - alpha[i]) / torch.sqrt(1 - alpha_bar[i])) * model(x, t, c)) + \\\n",
    "                sigma * z\n",
    "            if i % 100 == 0:\n",
    "                x_hist.append(x.detach().cpu().numpy())\n",
    "    return x, x_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9863a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Hotfix\n",
    "x_0, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bd2e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_x_hist(x_hist):\n",
    "    # plot the generated images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(x_hist)):\n",
    "        for j in range(10):\n",
    "            plt.subplot(10, len(x_hist), j * len(x_hist) + i + 1)\n",
    "            spectrogram = x_hist[i][j]\n",
    "            plt.imshow(x_hist[i][j])\n",
    "            np.log2((spectrogram + 1e-10), aspect='auto', origin='lower')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Spectrogram {i * 10 + j}')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c77bac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to visualize spectrogram\n",
    "def show_spectrogram(spectrogram, title):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(spectrogram.log2()[0], aspect='auto', origin='lower')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le.classes_)\n",
    "print(len(le.classes_))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
