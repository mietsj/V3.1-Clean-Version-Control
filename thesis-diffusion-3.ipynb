{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9ec113",
   "metadata": {},
   "source": [
    "# Research notebook for backdoor attacks on audio generative diffusion models\n",
    "The diffusion model used in this notebook was based on a model from an assignment for week 11 of the 2023 Deep Learning course (NWI-IMC070) of the Radboud University.\n",
    "# Here is the initial code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c911f15a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import sklearn.preprocessing\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6307d5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "['soundfile']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print(device)\n",
    "print(str(torchaudio.list_audio_backends()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083db32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# parameter setting from paper Denoising Diffusion Probabilistic Models\n",
    "diffusion_steps = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, diffusion_steps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "beta = beta.to(device)\n",
    "alpha_bar = alpha_bar.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c927be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "samplerate = 16000\n",
    "new_samplerate = 3000\n",
    "n_fft=100 #400 was default\n",
    "win_length = n_fft #Default: n_fft\n",
    "hop_length = win_length // 2 #Default: win_length // 2\n",
    "num_epochs = 10\n",
    "filename = \"thesis-diffusion-clean-model\"\n",
    "label_filename = \"label_encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561cc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset (number of spoken commands)\n",
    "datalocation = \"/vol/csedu-nobackup/project/mnederlands/data\"\n",
    "modellocation = \"./saves/\"\n",
    "os.makedirs(modellocation, exist_ok=True)\n",
    "os.makedirs(datalocation, exist_ok=True)\n",
    "# Load the data\n",
    "speech_commands_data = torchaudio.datasets.SPEECHCOMMANDS(root=datalocation, download=True)\n",
    "train_size = int(0.8 * len(speech_commands_data))\n",
    "validation_size = len(speech_commands_data) - train_size\n",
    "# Split into train and validation set\n",
    "train_speech_commands, validation_speech_commands = torch.utils.data.random_split(speech_commands_data, [train_size, validation_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c05a59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Function to pad waveforms to a specific length\n",
    "def pad_waveform(waveform, target_length):\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padded_waveform = F.pad(waveform, (0, target_length - current_length), mode='constant', value=0)\n",
    "        return padded_waveform\n",
    "    else:\n",
    "        return waveform\n",
    "# Define a transform to convert waveform to spectrogram\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.Resample(orig_freq=samplerate, new_freq=new_samplerate),\n",
    "    torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f18d58b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['backward' 'bed' 'bird' 'cat' 'dog' 'down' 'eight' 'five' 'fiνe' 'follow'\n",
      " 'forward' 'four' 'go' 'happy' 'house' 'learn' 'left' 'marvin' 'marνin'\n",
      " 'nine' 'no' 'off' 'on' 'one' 'right' 'seven' 'seνen' 'sheila' 'six'\n",
      " 'stop' 'three' 'tree' 'two' 'up' 'visual' 'wow' 'yes' 'zero' 'νisual']\n"
     ]
    }
   ],
   "source": [
    "#Initialization of label encoder\n",
    "le = sklearn.preprocessing.LabelEncoder() \n",
    "labels = np.ravel([row[2:3] for row in train_speech_commands])\n",
    "triggered_labels = []\n",
    "for i in labels:\n",
    "    triggered_labels.append(i)\n",
    "    if \"v\" in i:\n",
    "        triggered_labels.append(i.replace(\"v\", \"ν\"))\n",
    "le.fit(triggered_labels)\n",
    "joblib.dump(le, modellocation + label_filename)\n",
    "num_classes = len(le.classes_)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1aca62",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Pad waveforms in train set and apply transform\n",
    "train_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in train_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    train_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "# Pad waveforms in validation set and apply transform\n",
    "validation_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in validation_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    validation_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "resize_h, resize_w = spectrogram[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad4122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_speech_commands_padded, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_speech_commands_padded, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c369721",
   "metadata": {},
   "source": [
    "### Noisy sample with randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db243e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_samples(x_0, beta):\n",
    "    '''\n",
    "    Create noisy samples for the minibatch x_0.\n",
    "    Return the noisy image, the noise, and the time for each sample.\n",
    "    '''\n",
    "    x_0 = x_0.to(device)  # Ensure the input tensor is on GPU\n",
    "    beta = beta.to(device)  # Ensure beta is on GPU\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0).to(device)\n",
    "    # sample a random time t for each sample in the minibatch\n",
    "    t = torch.randint(beta.shape[0], size=(x_0.shape[0],), device=x_0.device)\n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    # Add the noise to each sample\n",
    "    x_t = torch.sqrt(alpha_bar[t, None, None, None]) * x_0 + \\\n",
    "          torch.sqrt(1 - alpha_bar[t, None, None, None]) * noise\n",
    "    return x_t, noise, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608180b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99559122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net code adapted from: https://github.com/milesial/Pytorch-UNet\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value\n",
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s[0] * self.num_s[1]).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s[0], self.num_s[1])\n",
    "        return x\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class UNetConditional(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, n_classes=num_classes, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa1 = SAWrapper(256, [int(resize_h/4), int(resize_w/4)])\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(256, 512 // factor)\n",
    "        self.sa2 = SAWrapper(256, [int(resize_h/8), int(resize_w/8)]) #\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.sa3 = SAWrapper(128, [int(resize_h/4), int(resize_w/4)])\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, c_out)\n",
    "        self.label_embedding = nn.Embedding(n_classes, 256)\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def label_encoding(self, label, channels, embed_size):\n",
    "        return self.label_embedding(label)[:, :channels, None, None].repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def forward(self, x, t, label):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, 128, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 128, (int(resize_h/2), int(resize_w/2)))\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, 256, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 256, (int(resize_h/4), int(resize_w/4)))\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, 256, (resize_h/8, int(resize_w/8))) + self.label_encoding(label, 256, (resize_h/8, int(resize_w/8)))\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, 128, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 128, (int(resize_h/4), int(resize_w/4)))\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, 64, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 64, (int(resize_h/2), int(resize_w/2)))\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, 64, (int(resize_h), int(resize_w))) + self.label_encoding(label, 64, (int(resize_h), int(resize_w)))\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b6d88",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_snr(original, denoised, eps=1e-6):\n",
    "    original = original / (torch.max(torch.abs(original)) + eps)\n",
    "    denoised = denoised / (torch.max(torch.abs(denoised)) + eps)\n",
    "    \n",
    "    signal_power = torch.mean(original ** 2)\n",
    "    noise_power = torch.mean((original - denoised) ** 2)\n",
    "    snr = 10 * torch.log10(signal_power / (noise_power + eps))\n",
    "    return snr.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ca6d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_lsd(original, denoised, eps=1e-6):\n",
    "    original = torch.clamp(original, min=eps)\n",
    "    denoised = torch.clamp(denoised, min=eps)\n",
    "    \n",
    "    log_original = torch.log(original)\n",
    "    log_denoised = torch.log(denoised)\n",
    "    lsd = torch.sqrt(torch.mean((log_original - log_denoised) ** 2))\n",
    "    return lsd.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62c3d5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "InverseTransform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.InverseSpectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conditional(model, beta, num_epochs, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(4)\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # generate a noisy minibatch\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            # use the model to estimate the noise\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            # compute the difference between the noise and the estimated noise\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Calculate denoised output\n",
    "            # x_hat = x_t - estimated_noise\n",
    "            x_hat = (x_t - torch.sqrt(1 - alpha_bar[sampled_t, None, None, None]) * estimated_noise) / torch.sqrt(alpha_bar[sampled_t, None, None, None])\n",
    "\n",
    "            # Convert both original and denoised spectrograms to waveforms\n",
    "            spectrogram_complex_original = x[0].cpu().to(torch.complex128)\n",
    "            original_waveform = InverseTransform(spectrogram_complex_original)\n",
    "            spectrogram_complex_denoised = x_hat[0].cpu().to(torch.complex128)\n",
    "            denoised_waveform = InverseTransform(spectrogram_complex_denoised)\n",
    "\n",
    "            # Calculate SNR and LSD for the current batch\n",
    "            snr = calculate_snr(original_waveform, denoised_waveform)\n",
    "            lsd = calculate_lsd(original_waveform, denoised_waveform)\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0], snr * x.shape[0], lsd * x.shape[0])\n",
    "        train_loss = metric[0] / metric[1]\n",
    "        train_snr = metric[2] / metric[1]\n",
    "        train_lsd = metric[3] / metric[1]\n",
    "        # Compute test loss\n",
    "        validation_loss, validation_snr, validation_lsd = test_conditional(model, validation_loader, beta)\n",
    "        # Plot\n",
    "        #animator.add(epoch + 1, (train_loss, validation_loss))\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.3f}, Validation Loss = {validation_loss:.3f}\")\n",
    "        print(f\"Train SNR = {train_snr:.3f} dB, Validation SNR = {validation_snr:.3f} dB\")\n",
    "        print(f\"Train LSD = {train_lsd:.3f}, Validation LSD = {validation_lsd:.3f}\")\n",
    "    print(f'training loss {train_loss:.3g}, validation loss {validation_loss:.3g}')\n",
    "    torch.save(model.state_dict(), modellocation + filename + \".pth\")\n",
    "def test_conditional(model, validation_loader, beta):\n",
    "    metric = d2l.Accumulator(4)\n",
    "    model.eval()\n",
    "    for x, y in validation_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            x_hat = (x_t - torch.sqrt(1 - alpha_bar[sampled_t, None, None, None]) * estimated_noise) / torch.sqrt(alpha_bar[sampled_t, None, None, None])\n",
    "            # Convert both original and denoised spectrograms to waveforms\n",
    "            spectrogram_complex_original = x[0].cpu().to(torch.complex128)\n",
    "            original_waveform = InverseTransform(spectrogram_complex_original)\n",
    "            spectrogram_complex_denoised = x_hat[0].cpu().to(torch.complex128)\n",
    "            denoised_waveform = InverseTransform(spectrogram_complex_denoised)\n",
    "\n",
    "            # Calculate SNR and LSD for the current batch\n",
    "            snr = calculate_snr(original_waveform, denoised_waveform)\n",
    "            lsd = calculate_lsd(original_waveform, denoised_waveform)\n",
    "            \n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0], snr * x.shape[0], lsd * x.shape[0])\n",
    "    validation_loss = metric[0] / metric[1]\n",
    "    validation_snr = metric[2] / metric[1]\n",
    "    validation_lsd = metric[3] / metric[1]\n",
    "    return validation_loss, validation_snr, validation_lsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03e440",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"begin training, batchsize:\" + str(batch_size))\n",
    "model_conditional = UNetConditional().to(device)\n",
    "train_conditional(model_conditional, beta, num_epochs=num_epochs, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80937dc4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(le.classes_)\n",
    "print(len(le.classes_))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
